#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Dexter Eval Runner - è©•æ¸¬åŸ·è¡Œå™¨
è² è²¬åŸ·è¡Œè‡ªå‹•åŒ–è©•æ¸¬ä¸¦ç”¢å‡ºå ±å‘Š
"""

import asyncio
import json
import logging
import time
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Any

from tradingagents.meeting.orchestrator import MeetingOrchestrator
from .eval_models import TestCaseResult, EvalReport

logger = logging.getLogger("dexter_evals")

class DexterEvalRunner:
    def __init__(self, test_cases_path: str = "tests/dexter_evals/test_cases.json"):
        self.test_cases_path = Path(test_cases_path)
        self.orchestrator = MeetingOrchestrator()
        
    def load_test_cases(self) -> List[Dict[str, Any]]:
        with open(self.test_cases_path, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("test_cases", [])

    async def run_case(self, case: Dict[str, Any]) -> TestCaseResult:
        """åŸ·è¡Œå–®å€‹æ¸¬è©¦æ¡ˆä¾‹"""
        start_time = time.time()
        test_id = case["id"]
        logger.info(f"ğŸš€ æ­£åœ¨åŸ·è¡Œæ¸¬è©¦æ¡ˆä¾‹: {test_id} - {case['query']}")
        
        # æ¨¡æ“¬ç‹€æ…‹
        success = False
        error_msg = None
        tool_usage = []
        
        try:
            # é€™è£¡èª¿ç”¨ orchestrator ä½†ä¸ä½¿ç”¨ WebSocket emit
            # æœªä¾†å¯ä»¥é€²ä¸€æ­¥å„ªåŒ–ç‚º MockLLM æ¨¡å¼ä»¥ç¯€çœæˆæœ¬
            scratchpad = await self.orchestrator._create_and_execute_plan(
                query=case["query"],
                market=case["market"],
                symbol_key=case["symbol_key"],
                emit=None # Headless mode
            )
            
            if scratchpad:
                success = True
                # æ”¶é›†å¯¦éš›èª¿ç”¨çš„å·¥å…·
                tool_results = scratchpad.get_all_tool_results()
                tool_usage = [res.source_provider for res in tool_results.values()]
        except Exception as e:
            logger.error(f"âŒ æ¸¬è©¦æ¡ˆä¾‹ {test_id} å‡ºéŒ¯: {e}")
            error_msg = str(e)

        duration = (time.time() - start_time) * 1000
        
        # è¨ˆç®—åˆ†æ•¸
        scores = self._calculate_scores(case, tool_usage)
        
        return TestCaseResult(
            test_id=test_id,
            query=case["query"],
            symbol_key=case["symbol_key"],
            success=success,
            duration_ms=duration,
            total_steps=len(tool_usage),
            tool_usage=tool_usage,
            tool_accuracy_score=scores["accuracy"],
            reasoning_score=scores["reasoning"],
            overall_score=scores["overall"],
            error=error_msg
        )

    def _calculate_scores(self, case: Dict[str, Any], actual_tools: List[str]) -> Dict[str, float]:
        """ç°¡å–®çš„è©•åˆ†é‚è¼¯"""
        expected = set(case.get("expected_tools", []))
        actual = set(actual_tools)
        
        if not expected:
            accuracy = 100.0
        else:
            # å‘½ä¸­äº¤é›†æ¯”ä¾‹
            hit_count = len(expected.intersection(actual))
            accuracy = (hit_count / len(expected)) * 100.0
            
        # æ¨ç†åˆ†æ•¸ (æš«å®š: æœ‰èª¿ç”¨å·¥å…·å³ç‚ºåŸºæœ¬æ¨ç†æˆåŠŸ)
        reasoning = 100.0 if len(actual) > 0 else 0.0
        
        # ç¸½åˆ†åŠ æ¬Š
        overall = (accuracy * 0.7) + (reasoning * 0.3)
        
        return {
            "accuracy": accuracy,
            "reasoning": reasoning,
            "overall": overall
        }

    async def run_all(self) -> EvalReport:
        cases = self.load_test_cases()
        results = []
        start_time = datetime.now()
        
        for case in cases:
            result = await self.run_case(case)
            results.append(result)
            
        end_time = datetime.now()
        total = len(results)
        passed = sum(1 for r in results if r.success and r.overall_score >= 80)
        avg_score = sum(r.overall_score for r in results) / total if total > 0 else 0
        
        report = EvalReport(
            report_id=f"eval_{end_time.strftime('%Y%m%d_%H%M%S')}",
            start_time=start_time,
            end_time=end_time,
            total_cases=total,
            passed_cases=passed,
            failed_cases=total - passed,
            average_score=avg_score,
            results=results
        )
        
        self.save_report(report)
        return report

    def save_report(self, report: EvalReport):
        report_path = Path(f"tests/dexter_evals/reports/{report.report_id}.json")
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report.json(ensure_ascii=False, indent=2))
        
        # åŒæ­¥ç”Ÿæˆ Markdown æ‘˜è¦
        md_path = report_path.with_suffix(".md")
        with open(md_path, "w", encoding="utf-8") as f:
            f.write(f"# Dexter è©•æ¸¬å ±å‘Š: {report.report_id}\n\n")
            f.write(f"- **æ™‚é–“**: {report.start_time} - {report.end_time}\n")
            f.write(f"- **ç¸½æ¡ˆä¾‹**: {report.total_cases}\n")
            f.write(f"- **é€šç‡é**: {report.passed_cases}/{report.total_cases}\n")
            f.write(f"- **å¹³å‡åˆ†**: {report.average_score:.2f}\n\n")
            f.write("## è©³ç´°çµæœ\n\n")
            f.write("| ID | æŸ¥è©¢ | åˆ†æ•¸ | ç‹€æ…‹ | å·¥å…·ä½¿ç”¨ |\n")
            f.write("|---|---|---|---|---|\n")
            for r in report.results:
                status = "âœ…" if r.success and r.overall_score >= 80 else "âŒ"
                tools = ", ".join(r.tool_usage)
                f.write(f"| {r.test_id} | {r.query} | {r.overall_score:.1f} | {status} | {tools} |\n")

        logger.info(f"ğŸ“Š è©•æ¸¬å ±å‘Šå·²å­˜å„²: {report_path}")

if __name__ == "__main__":
    # ç°¡å–®çš„å…¥å£
    logging.basicConfig(level=logging.INFO)
    runner = DexterEvalRunner()
    asyncio.run(runner.run_all())
